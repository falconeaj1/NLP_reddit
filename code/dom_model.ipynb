{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5501e98-88fe-4154-a8c9-6254e495369d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba474556-c493-40a4-9ba1-024f596ea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ba8155-197f-4cd3-ab26-d732cc5cc308",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e774162b-72c0-48a0-9bd9-2d4220cc1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/reddit_dominion_full.csv')\n",
    "y = df['commments_greater']\n",
    "X = df['title']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f88468-7c68-4292-8173-79509a60f06f",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c45f98-a81d-4222-9280-3f8d12a123f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.518607\n",
       "1    0.481393\n",
       "Name: commments_greater, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb539f8b-99ab-4a14-b824-520f6895bab5",
   "metadata": {},
   "source": [
    "### If the model predicted majority class (less than or equal to 1 comment), the model would be right 51% of the time. The goal is to improve upon this baseline accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0679cf-5d49-4e8c-8a59-58c7d55b82d6",
   "metadata": {},
   "source": [
    "## Definte lemmatization and stemming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b23f06-07d2-4b2b-a813-82129a61dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tokenizer(doc):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "def stemmer_tokenizer(doc):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [ps.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edf4c7-9244-4898-bb72-20560c9547d5",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c90d4-41d4-4be9-8c2c-4b0f6fd8c017",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2049d85f-d876-43c7-8e22-31acc5413194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2)}\n",
      "Best Estimator Score Train:  0.8625468164794008\n",
      "Best Estimator Score Test:  0.5861875350926445\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([    \n",
    "    ('cv', CountVectorizer(tokenizer = lemma_tokenizer, min_df=3)),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    # 'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1),(1,2), (2,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc761d-a5d3-4465-be5b-dd6e6e5864ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a2020e-82d6-4d35-b0c3-d03c31881a6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TFID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092d4926-0c4a-4ac2-9414-c838884e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'tfid__ngram_range': (1, 1), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.7290262172284644\n",
      "Best Estimator Score Test:  0.5794497473329591\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   4.1s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   1.9s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   1.5s\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=   2.2s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time=   3.1s\n",
      "[CV] END ....tfid__ngram_range=(2, 2), tfid__stop_words=None; total time=   2.8s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   3.8s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   2.0s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.7s\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=   2.3s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time=   3.0s\n",
      "[CV] END ....tfid__ngram_range=(2, 2), tfid__stop_words=None; total time=   2.7s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   4.0s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   2.0s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.5s\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=   2.3s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time=   3.0s\n",
      "[CV] END ....tfid__ngram_range=(2, 2), tfid__stop_words=None; total time=   2.9s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   4.0s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   2.0s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   1.7s\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=   2.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time=   3.0s\n",
      "[CV] END ....tfid__ngram_range=(2, 2), tfid__stop_words=None; total time=   2.8s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   3.9s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   2.1s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   1.7s\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=   2.3s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time=   3.1s\n",
      "[CV] END ....tfid__ngram_range=(2, 2), tfid__stop_words=None; total time=   2.8s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   4.0s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   1.9s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.7s\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=   2.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time=   3.0s\n",
      "[CV] END .tfid__ngram_range=(2, 2), tfid__stop_words=english; total time=   2.8s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   3.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   2.0s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   1.7s\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=   2.4s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time=   3.0s\n",
      "[CV] END .tfid__ngram_range=(2, 2), tfid__stop_words=english; total time=   2.9s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   4.1s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   2.2s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   1.5s\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=   2.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time=   3.0s\n",
      "[CV] END .tfid__ngram_range=(2, 2), tfid__stop_words=english; total time=   2.8s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   4.0s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   2.0s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.7s\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=   2.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time=   3.1s\n",
      "[CV] END .tfid__ngram_range=(2, 2), tfid__stop_words=english; total time=   2.7s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   4.0s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   1.9s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.6s\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=   2.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time=   3.3s\n",
      "[CV] END .tfid__ngram_range=(2, 2), tfid__stop_words=english; total time=   2.7s\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([    \n",
    "    ('tfid', TfidfVectorizer(tokenizer=stemmer_tokenizer, min_df=3)),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1),(1,2), (2,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad0bb3-38eb-402c-ad3a-06b9d7158d11",
   "metadata": {},
   "source": [
    "# Naive Bayes (Bernouilli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9240b5-658a-4d48-bc8e-dfa123921f7e",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04d1a30e-ab8d-45e8-945a-3d20ae0f35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.8642540024216333\n",
      "Best Estimator Score Test:  0.5639714785416386\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b43c8-6b6c-4f78-9a9b-140cdb300d5f",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29bdab18-0c16-466a-a93a-72f6ab07c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.8642540024216333\n",
      "Best Estimator Score Test:  0.5639714785416386\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2341be-a4c5-4352-85b9-2dc5e86e1abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7562863-8b6f-4828-9784-8e45bca07a9a",
   "metadata": {},
   "source": [
    "# Naive Bayes (Multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a804a01-5b6a-498a-b516-7f38cf2f0bb2",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "81a95cd1-c3c6-455d-acb6-400d20888bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Params:  {}\n",
      "Best Estimator Score Train:  0.696578321897843\n",
      "Best Estimator Score Test:  0.5596663527512444\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nbm', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid={}, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2238c23-e17d-471d-8692-9651a53f2529",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42d3c682-7cd2-439b-b2e4-8d88c1325729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 3), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.9365890847123189\n",
      "Best Estimator Score Test:  0.5828064038746132\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('nbm', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231dea3-8561-4b78-ad34-95498af309df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132f8cd5-39c1-4f35-b35b-5b6d8fc90333",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52457db-14c1-4d16-9d25-e1b90dfedf03",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "630d049f-1827-4be0-9d42-0ffcba98774a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.9819274406924078\n",
      "Best Estimator Score Test:  0.564509619265438\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde21569-9379-4aa2-a472-b66fa8b33667",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da80db2d-97c2-4c9c-a95f-96d02e3dc40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.9819274406924078\n",
      "Best Estimator Score Test:  0.5635678729987892\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.5s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.4min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.4s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.5min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.3s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.6min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.7s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.5min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.4s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.7min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  39.0s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.4min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  37.9s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  38.5s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.2min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.6min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  38.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  39.1s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff6baa-ecab-469e-8968-1e6c914d2ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24e5f062-8b19-4587-a32f-6cc14f0a67b7",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d74675-3a51-4ae0-89f0-efd31073af5e",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c41325dc-a935-462c-9f40-8a99c32f8f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.59262747208395\n",
      "Best Estimator Score Test:  0.5595318175702947\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fd0e5-68a7-42b3-a914-ea60a5b2bd44",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adca6715-54b7-4c93-82b0-aef1217b484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 1), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.6080990178931791\n",
      "Best Estimator Score Test:  0.5575137898560474\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ace57db-07af-4068-a861-5eb9097bf5d5",
   "metadata": {},
   "source": [
    "# Ada BoostGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23a3bb-2d98-4710-8d1c-c46059f1c7fa",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a1f4712-7f82-4a45-a7c9-ab09447d1216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 1)}\n",
      "Best Estimator Score Train:  0.6316479400749063\n",
      "Best Estimator Score Test:  0.5586749017405952\n",
      "[CV] END .............................cv__ngram_range=(1, 2); total time=   3.0s\n",
      "[CV] END .............................cv__ngram_range=(1, 2); total time=   3.4s\n",
      "[CV] END .............................cv__ngram_range=(1, 2); total time=   3.5s\n",
      "[CV] END .............................cv__ngram_range=(1, 2); total time=   3.6s\n",
      "[CV] END .............................cv__ngram_range=(1, 2); total time=   3.5s\n",
      "[CV] END .............................cv__ngram_range=(1, 1); total time=   2.9s\n",
      "[CV] END .............................cv__ngram_range=(1, 3); total time=   2.4s\n",
      "[CV] END .............................cv__ngram_range=(1, 1); total time=   2.8s\n",
      "[CV] END .............................cv__ngram_range=(1, 3); total time=   2.4s\n",
      "[CV] END .............................cv__ngram_range=(1, 1); total time=   2.9s\n",
      "[CV] END .............................cv__ngram_range=(1, 3); total time=   2.4s\n",
      "[CV] END .............................cv__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END .............................cv__ngram_range=(1, 3); total time=   2.4s\n",
      "[CV] END .............................cv__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END .............................cv__ngram_range=(1, 3); total time=   2.4s\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer(tokenizer=lemma_tokenizer)),\n",
    "    ('tf', TfidfTransformer()),\n",
    "    ('ab', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    # 'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4836738-4080-4cab-b75e-8ceeeed57e21",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96db5723-86d4-45d6-867e-b88edb6dfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.5749136732588905\n",
      "Best Estimator Score Test:  0.5490380734562088\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('ab', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265661d8-9e02-47fe-b546-71c24014b322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd299914-bd02-4495-b401-bbbc982c850d",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76973635-fe03-4fec-baf4-151e0845a161",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "981c9c2c-a905-4a62-981b-0667c4e49ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.7046504327548321\n",
      "Best Estimator Score Test:  0.5292614018565855\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2df5d-6cb9-4e2e-aced-50dd85b05770",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8caf9d6a-8b0b-4b6d-acfd-0803b32acabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 1), 'tfid__stop_words': 'english'}\n",
      "Best Estimator Score Train:  0.7016458137136194\n",
      "Best Estimator Score Test:  0.5315484999327325\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dda3b-5d29-44d7-b039-3ffd1c79e875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
