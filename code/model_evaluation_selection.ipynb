{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a265d45-020e-4044-a5b2-a15c1be6c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import f1_score, confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a92a49-ab59-4c0a-b7f4-48e65506e820",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "949690d9-de87-435a-b12f-f27246e6c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/reddit_chessbeginners_full.csv')\n",
    "y = df['comm_greater_1']\n",
    "X = df['title']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3271a82c-2f19-47f6-a1f2-7db01412bdf6",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1d65de8-f4df-47c1-a8c2-ca92d59f6fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.510359\n",
       "1    0.489641\n",
       "Name: comm_greater_1, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a55a76-f515-4ae8-8f7c-38f50b39e243",
   "metadata": {},
   "source": [
    "### If the model predicted majority class (less than or equal to 1 comment), the model would be right 51% of the time. The goal is to improve upon this baseline accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b77b2e-090e-4f4f-a66e-c6babaa7c522",
   "metadata": {},
   "source": [
    "## Definte lemmatization and stemming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f78fb5cd-d535-4eda-ba8c-d30b21ed4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tokenizer(doc):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [wnl.lemmatize(t) for t in tokens]\n",
    "\n",
    "def stemmer_tokenizer(doc):\n",
    "    ps = PorterStemmer()\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [ps.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d126262-24db-4e53-bde7-07a96f17b048",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1219ac9-e322-473f-a493-8988957338c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d08b74f-f06d-4cc5-84d7-4348320647cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/andrewfalcone/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': 'english'}\n",
      "Best Estimator Score Train:  0.6695815955872461\n",
      "Best Estimator Score Test:  0.5558993676846495\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   0.7s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   1.6s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   0.8s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   5.6s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   8.3s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=  10.0s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   0.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   0.9s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.2s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   6.2s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   7.8s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=  10.0s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   0.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   0.8s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.2s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   6.2s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   8.3s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   9.6s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   0.7s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   1.6s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   0.8s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   6.1s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   8.4s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   9.7s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   0.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   0.9s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.3s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   6.2s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   8.1s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=  10.0s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   0.7s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   1.7s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   0.8s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   6.1s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   8.5s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   9.6s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   0.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   1.0s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.2s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   5.9s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   9.0s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   9.4s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   0.7s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   1.5s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   0.9s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   6.0s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   8.7s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   9.5s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   0.7s\n",
      "[CV] END ........cv__ngram_range=(1, 2), cv__stop_words=None; total time=   1.7s\n",
      "[CV] END .....cv__ngram_range=(2, 2), cv__stop_words=english; total time=   0.8s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   6.2s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   8.1s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=  10.1s\n",
      "[CV] END ........cv__ngram_range=(1, 1), cv__stop_words=None; total time=   0.9s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   1.1s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=   1.2s\n",
      "[CV] END .....cv__ngram_range=(1, 1), cv__stop_words=english; total time=   6.1s\n",
      "[CV] END .....cv__ngram_range=(1, 2), cv__stop_words=english; total time=   7.8s\n",
      "[CV] END ........cv__ngram_range=(2, 2), cv__stop_words=None; total time=  10.5s\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([    \n",
    "    ('cv', CountVectorizer(tokenizer = lemma_tokenizer, min_df=3)),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1),(1,2), (2,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41980a4d-a492-476e-b859-dc5fc6a89d1d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5dadee7-4bb7-41fa-b5a4-5a96eeba1d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TFID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c6d079-25ec-4f03-8d11-fa5ef413d2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.8443876407013767\n",
      "Best Estimator Score Test:  0.5748688281985739\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([    \n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1),(1,2), (2,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a2673-254d-4fc2-bbdb-bf9b578fa888",
   "metadata": {},
   "source": [
    "# Naive Bayes (Bernouilli)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcb2cc-cd2b-45ca-94fe-66cf39c40010",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee2d56e-76a2-4e97-afa3-ae00ef318a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.8642540024216333\n",
      "Best Estimator Score Test:  0.5639714785416386\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79b0be-bce7-4eba-904d-069bb466ae75",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ea0c0e1-292c-4983-9a82-5672f9bd41eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.8642540024216333\n",
      "Best Estimator Score Test:  0.5639714785416386\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82b68b7-a7f6-44e5-9d7f-1e7c8e95d4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b023ad1b-c7bc-424c-842f-8ed21ae01d05",
   "metadata": {},
   "source": [
    "# Naive Bayes (Multinomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f97130-0a1d-430d-8f0e-2c3b1166a455",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63ceeddd-6eaf-4915-98ab-40f231322e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Params:  {}\n",
      "Best Estimator Score Train:  0.696578321897843\n",
      "Best Estimator Score Test:  0.5596663527512444\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('nbm', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid={}, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6374ed2e-e0d7-4c81-9fe9-4f57300e42ce",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4322f3bc-e53b-4300-ab08-ed31e58b6925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 3), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.9365890847123189\n",
      "Best Estimator Score Test:  0.5828064038746132\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('nbm', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab3889-62f4-45af-9f22-5f3bb83c1992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "591124dc-3936-49f5-bbcc-e63e50966f82",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ffa104-1323-4225-898a-40cc14f22411",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aeac698f-c2de-4d3d-bfc9-dc28392efadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.9819274406924078\n",
      "Best Estimator Score Test:  0.564509619265438\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf4141-0e65-4eca-b334-8cb3358d071f",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "879632ae-a4c0-43e9-b5ae-f31e9f77795a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.9819274406924078\n",
      "Best Estimator Score Test:  0.5635678729987892\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.5s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.4min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.4s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.5min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.3s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.6min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.7s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.5min\n",
      "[CV] END ....tfid__ngram_range=(1, 1), tfid__stop_words=None; total time=  36.4s\n",
      "[CV] END ....tfid__ngram_range=(1, 2), tfid__stop_words=None; total time= 2.4min\n",
      "[CV] END .tfid__ngram_range=(1, 3), tfid__stop_words=english; total time= 4.7min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  39.0s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.4min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  37.9s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  38.5s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.2min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.6min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  38.4s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n",
      "[CV] END .tfid__ngram_range=(1, 1), tfid__stop_words=english; total time=  39.1s\n",
      "[CV] END .tfid__ngram_range=(1, 2), tfid__stop_words=english; total time= 2.3min\n",
      "[CV] END ....tfid__ngram_range=(1, 3), tfid__stop_words=None; total time= 5.5min\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077863dd-eb03-4b05-be74-3975f7af039a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "785cabc2-5b72-4aed-b0ec-4203561f0ec9",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4819b00-d3b1-4aa8-bdfb-04dfbad8512c",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c02ae39-5a4e-4e49-aae0-f295ee01f81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.59262747208395\n",
      "Best Estimator Score Test:  0.5595318175702947\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf137e15-5047-4c93-b90f-230935cc54e1",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3056aef0-0c10-4476-976f-4716dc0407ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 1), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.6080990178931791\n",
      "Best Estimator Score Test:  0.5575137898560474\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89722c35-c9f7-4ae5-8667-2708426ca9a5",
   "metadata": {},
   "source": [
    "# Ada BoostGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a33ce-fc49-428a-be52-e51a50eaac9d",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e76542d-30ab-46c3-b0f2-6451114dcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 2), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.572133279519261\n",
      "Best Estimator Score Test:  0.5505179604466568\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('ab', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30acb55-1a63-47ca-8166-83a8134214c5",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37cd9cad-d1b6-47bb-959f-0c20b2d8fdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 2), 'tfid__stop_words': None}\n",
      "Best Estimator Score Train:  0.5749136732588905\n",
      "Best Estimator Score Test:  0.5490380734562088\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('ab', AdaBoostClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5b97e-6d98-4360-bd9d-73257b1bf64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d154877-bd02-4b88-b9b2-25030e88e7f6",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb3b7e-93db-4917-ac98-b7dd6494cccd",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e09f67c8-ca4e-41f0-9620-7d468afe3af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'cv__ngram_range': (1, 1), 'cv__stop_words': None}\n",
      "Best Estimator Score Train:  0.7046504327548321\n",
      "Best Estimator Score Test:  0.5292614018565855\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cv', CountVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'cv__stop_words' : [None, 'english'],\n",
    "    'cv__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6bb552-8a50-494b-9cc9-9930078b9c4b",
   "metadata": {},
   "source": [
    "### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ef9cc05-a701-4764-a185-a5d3e6d44370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Params:  {'tfid__ngram_range': (1, 1), 'tfid__stop_words': 'english'}\n",
      "Best Estimator Score Train:  0.7016458137136194\n",
      "Best Estimator Score Test:  0.5315484999327325\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('tfid', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tfid__stop_words' : [None, 'english'],\n",
    "    'tfid__ngram_range' : [(1,1), (1,2), (1,3)],\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ', gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ac0b8-7e09-42fa-85e3-92b810f6e6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
